<!doctypehtml><html lang="zh-Hant"class="scroll-smooth"><meta name="twitter:image"content="https://wellstsai.com/single-page-conclusion/%E5%AF%A6%E9%AB%94AI-NVIDIA%E7%9A%84%E6%AC%A1%E4%B8%96%E4%BB%A3%E5%B7%A5%E6%A5%AD%E6%95%B8%E4%BD%8D%E5%8C%96%E7%AD%96%E7%95%A5.png"><meta name="twitter:description"content="探討NVIDIA「實體AI」第四波浪潮。說明其運用Blackwell架構、GROOT模型與Omniverse數位孿生，並透過Isaac及Metropolis兩大路徑，推動價值50兆美元的工業革命。"><meta name="twitter:title"content="實體AI-NVIDIA的次世代工業數位化策略"><meta name="twitter:card"content="summary_large_image"><meta property="og:type"content="article"><meta property="og:site_name"content="WellWells"><meta property="og:image"content="https://wellstsai.com/single-page-conclusion/%E5%AF%A6%E9%AB%94AI-NVIDIA%E7%9A%84%E6%AC%A1%E4%B8%96%E4%BB%A3%E5%B7%A5%E6%A5%AD%E6%95%B8%E4%BD%8D%E5%8C%96%E7%AD%96%E7%95%A5.png"><meta property="og:url"content="https://wellstsai.com/single-page-conclusion/%E5%AF%A6%E9%AB%94AI-NVIDIA%E7%9A%84%E6%AC%A1%E4%B8%96%E4%BB%A3%E5%B7%A5%E6%A5%AD%E6%95%B8%E4%BD%8D%E5%8C%96%E7%AD%96%E7%95%A5.html"><meta property="og:description"content="探討NVIDIA「實體AI」第四波浪潮。說明其運用Blackwell架構、GROOT模型與Omniverse數位孿生，並透過Isaac及Metropolis兩大路徑，推動價值50兆美元的工業革命。"><meta property="og:title"content="實體AI-NVIDIA的次世代工業數位化策略"><meta name="description"content="探討NVIDIA「實體AI」第四波浪潮。說明其運用Blackwell架構、GROOT模型與Omniverse數位孿生，並透過Isaac及Metropolis兩大路徑，推動價值50兆美元的工業革命。"><meta charset="UTF-8"><meta name="viewport"content="width=device-width,initial-scale=1"><title>實體 AI：引領次世代工業數位化浪潮 | NVIDIA 策略解析</title><link rel="preconnect"href="https://fonts.googleapis.com"><link rel="preconnect"href="https://fonts.gstatic.com"crossorigin><link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap"rel="stylesheet"><script src="https://cdn.tailwindcss.com"></script><link rel="stylesheet"href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0KOVEMmgEEKSUtqcTigvUv1eNfP6d7XnwUj3pSjs0BUSzMKxEbwPg"crossorigin="anonymous"><script defer="defer"src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGjKrxZSSFNjzccdbdNQPBIYxYFRnAFGZkM8/vSGUCCMHIQIAksUFoNdVK"crossorigin="anonymous"></script><script defer="defer"src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-+VBxd3r6XgURPlLJSzFGFgCFevM/hTunderCCWsf1adcfiUykkFXbeoEWYid+sSG"crossorigin="anonymous"></script><style>body{font-family:Roboto,'Noto Sans TC',sans-serif}.gradient-text{background-clip:text;-webkit-background-clip:text;-webkit-text-fill-color:transparent}.card{background-color:rgba(17,24,39,.7);border:1px solid rgba(55,65,81,.7);backdrop-filter:blur(12px);-webkit-backdrop-filter:blur(12px)}.keyword{border-bottom:2px dotted rgba(96,165,250,.7);cursor:help;transition:all .2s ease-in-out}.keyword:hover{background-color:rgba(59,130,246,.1)}#tooltip{position:absolute;z-index:50;padding:.75rem;border-radius:.5rem;box-shadow:0 10px 15px -3px rgba(0,0,0,.3),0 4px 6px -2px rgba(0,0,0,.15);transition:opacity .2s ease-in-out,visibility .2s ease-in-out;pointer-events:none;opacity:0;visibility:hidden;max-width:300px}@media (max-width:768px){#tooltip{max-width:90vw;font-size:.875rem;padding:.5rem}.card{padding:1rem!important}h1{font-size:1.875rem!important;line-height:2.25rem!important}h2{font-size:1.5rem!important;line-height:2rem!important}h3{font-size:1.25rem!important;line-height:1.75rem!important}table{font-size:.875rem}table td,table th{padding:.5rem!important}div,li,p{font-size:.9375rem;line-height:1.6}.space-y-8>*+*{margin-top:1.5rem!important}.space-y-6>*+*{margin-top:1rem!important}.text-5xl{font-size:2.5rem!important}blockquote{padding:.75rem!important;font-size:.875rem!important}}@media (min-width:768px) and (max-width:1024px){.container{padding-left:1.5rem;padding-right:1.5rem}}code{background-color:rgba(55,65,81,.5);color:#f0fdf4;padding:.125rem .375rem;border-radius:.25rem;font-size:.9em}.qa-link{color:#7dd3fc;text-decoration:underline;text-underline-offset:2px;transition:color .2s}.qa-link:hover{color:#e0f2fe}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-JKC4KZLT26"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JKC4KZLT26")</script><body class="bg-gradient-to-b from-gray-900 to-black text-slate-300 min-h-screen antialiased"><header class="container mx-auto px-4 md:px-6 pt-12 md:pt-20 pb-8 md:pb-12 text-center"><h1 class="text-3xl md:text-6xl font-bold mb-4">🚀 <span class="gradient-text bg-gradient-to-r from-sky-400 to-purple-400">實體 AI：引領次世代工業數位化浪潮</span></h1><p class="text-lg md:text-xl text-slate-400 max-w-3xl mx-auto"><span class="keyword"data-keyword="NVIDIA">NVIDIA</span> 揭示從晶片到 AI 基礎設施的轉型，驅動價值 50 兆美元的產業革命。</header><section class="container mx-auto px-4 md:px-6 max-w-3xl mb-12 md:mb-16"><div class="card rounded-lg shadow-2xl p-4 md:p-8"><h2 class="text-xl md:text-2xl font-bold mb-4 from-teal-300 to-blue-400">📝 核心摘要</h2><div class="space-y-3 text-slate-300 text-sm md:text-base"><p>NVIDIA 提出「<span class="keyword"data-keyword="Physical AI">實體 AI</span>」(Physical AI) 作為繼感知型、生成式 與代理型 AI 後的第四波浪潮。<p>此技術旨在讓 AI 理解物理定律並在現實世界中自主行動，將對製造、<span class="keyword"data-keyword="GROOT">機器人</span>、自動駕駛 等價值 50 兆美元的實體產業 帶來根本性變革。<p>其核心是一個能接收圖像、文字（如：「<strong class="text-sky-300">Ask me anything or tell me to do anything.</strong>」）等多模態輸入，並輸出「<strong>行動權杖</strong>」(Action Tokens) 的基礎模型。</div></div></section><main class="container mx-auto px-4 md:px-6 py-8 md:py-12 space-y-12 md:space-y-20 mb-16 md:mb-32"><section class="grid md:grid-cols-2 gap-10 items-center"><div class="space-y-4"><h2 class="text-3xl font-bold mb-4">💡 <span class="gradient-text bg-gradient-to-r from-sky-300 to-cyan-300">NVIDIA 的三位一體電腦架構</span></h2><p>NVIDIA 圍繞<span class="keyword"data-keyword="Physical AI">實體 AI</span>打造了三台專用電腦，皆基於最新的 <span class="keyword"data-keyword="Blackwell">Blackwell</span> 架構，構成一個完整的開發與部署閉環：<ul class="list-none space-y-3 pl-4"><li><strong class="block text-lg text-green-300">1. 訓練 (TRAIN)</strong><p class="text-slate-400">使用 <span class="keyword"data-keyword="GB200 NVL72">NVIDIA GB200 NVL72</span> 或 <span class="keyword"data-keyword="DGX/HGX">DGX/HGX</span> 進行大規模模型開發。<li><strong class="block text-lg text-blue-300">2. 模擬 (SIMULATE)</strong><p class="text-slate-400">使用 <span class="keyword"data-keyword="RTX PRO Server">NVIDIA RTX PRO Server</span> 運行 <span class="keyword"data-keyword="Omniverse">Omniverse</span> 與 <span class="keyword"data-keyword="Cosmos">Cosmos</span>，生成合成資料並進行物理模擬。<li><strong class="block text-lg text-purple-300">3. 部署 (DEPLOY)</strong><p class="text-slate-400">使用 <span class="keyword"data-keyword="Jetson Thor AGX">NVIDIA Jetson Thor AGX</span> 將訓練好的模型部署到終端裝置（如機器人）上。</ul></div><div class="card p-8 rounded-lg shadow-lg flex flex-col md:flex-row justify-around items-center gap-8 min-h-[200px]"><div class="text-center"><span class="text-5xl">🧠</span> <span class="block mt-2 font-bold text-lg text-green-300">TRAIN</span></div><div class="text-5xl text-slate-500 hidden md:block">→</div><div class="text-center"><span class="text-5xl">🔄</span> <span class="block mt-2 font-bold text-lg text-blue-300">SIMULATE</span></div><div class="text-5xl text-slate-500 hidden md:block">→</div><div class="text-center"><span class="text-5xl">🤖</span> <span class="block mt-2 font-bold text-lg text-purple-300">DEPLOY</span></div></div></section><section class="space-y-8"><h2 class="text-3xl font-bold text-center">🧠 <span class="gradient-text bg-gradient-to-r from-pink-300 to-orange-300">核心模型：GROOT 與 Cosmos</span></h2><p class="text-center max-w-2xl mx-auto text-slate-400">NVIDIA 提供一系列如 <span class="keyword"data-keyword="GROOT">GROOT</span>、<span class="keyword"data-keyword="Cosmos">Cosmos</span>、Llama 等預訓練模型 以加速開發。<div class="grid md:grid-cols-2 gap-8"><div class="card rounded-lg shadow-lg p-6 md:p-8 flex flex-col"><h3 class="text-2xl font-bold mb-3 text-lime-300">🤖 GROOT 人形機器人模型</h3><p class="mb-4">專為人形機器人設計的開放、可客製化基礎模型。它能理解多模態指令（如感測器 與文字），並轉化為機器人行動。<blockquote class="mt-auto bg-gray-800/60 border-l-4 border-lime-400 p-4 rounded-r-md text-slate-300 italic">"Pick up the industrial object and place in yellow bin."</blockquote></div><div class="card rounded-lg shadow-lg p-6 md:p-8 flex flex-col"><h3 class="text-2xl font-bold mb-3 text-sky-300">🌌 Cosmos 世界基礎模型</h3><p class="mb-4">世界基礎模型開發平台，用於推進<span class="keyword"data-keyword="Physical AI">實體 AI</span>對物理世界的理解與生成。<ul class="list-none space-y-2 mt-auto"><li><strong class="text-sky-300">Cosmos Predict:</strong> 從多模態輸入預測未來虛擬世界狀態。<li><strong class="text-sky-300">Cosmos Transfer:</strong> 生成受現實世界 與 3D 輸入 制約的物理感知虛擬世界。<li><strong class="text-sky-300">Cosmos Reason:</strong> 進行思維鏈推理(Chain-of-thought)，以理解<span class="keyword"data-keyword="Physical AI">實體 AI</span>的世界狀態。</ul></div></div></section><section class="grid md:grid-cols-2 gap-10 items-center"><div class="card p-8 rounded-lg shadow-lg flex justify-around items-center min-h-[200px] order-last md:order-first"><div class="text-center"><span class="text-5xl">📈</span> <span class="block mt-2 font-bold text-lg text-red-300">資料挑戰</span></div><div class="text-5xl text-slate-500">→</div><div class="text-center"><span class="text-5xl">🌍</span> <span class="block mt-2 font-bold text-lg text-green-300">數位孿生</span></div><div class="text-5xl text-slate-500">→</div><div class="text-center"><span class="text-5xl">🤖</span> <span class="block mt-2 font-bold text-lg text-yellow-300">Isaac Sim</span></div></div><div class="space-y-4"><h2 class="text-3xl font-bold mb-4">🌐 <span class="gradient-text bg-gradient-to-r from-green-300 to-yellow-300">數位孿生：Omniverse 與 Isaac Sim</span></h2><p><span class="keyword"data-keyword="Physical AI">實體 AI</span>面臨巨大的「機器人資料挑戰」。NVIDIA 透過「<strong>訓練後縮放</strong>」(Post-training Scaling) 來解決，即在物理模擬中生成合成資料。<ul class="list-none space-y-3 pl-4"><li><strong class="block text-lg text-green-300"><span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span></strong><p class="text-slate-400">基於 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span> 的工業數位化應用平台，用於建立和操作「<span class="keyword"data-keyword="Digital Twins">數位孿生</span>」(Digital Twins)。<li><strong class="block text-lg text-yellow-300"><span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span></strong><p class="text-slate-400">專為機器人模擬與測試而生，可生成合成資料 並進行 SIL/HIL 測試。</ul><p>兩者結合可產生可控的合成資料，是解鎖工業檢測、機器人機群模擬 等應用的關鍵。</div></section><section class="space-y-8"><h2 class="text-3xl font-bold text-center">🤖 <span class="gradient-text bg-gradient-to-r from-purple-300 to-indigo-300">兩大部署路徑：Isaac 與 Metropolis</span></h2><p class="text-center max-w-2xl mx-auto text-slate-400">NVIDIA 提供兩種將<span class="keyword"data-keyword="Physical AI">實體 AI</span>應用於設施的方法，分別專注於不同的應用場景：<div class="grid md:grid-cols-2 gap-8"><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-3 text-purple-300">1. 🤖 由內而外 (Inside-Out AI)</h3><p>以「<span class="keyword"data-keyword="Digital Twins">數位孿生</span>」為核心，使用 <span class="keyword"data-keyword="Isaac">NVIDIA Isaac</span> 平台。此路徑專注於自主硬體，如：<ul class="list-disc list-inside mt-2 space-y-1 text-slate-400"><li>自主移動機器人 (AMR)<li>機械手臂 (Manipulator)<li>人形機器人 (Humanoids)</ul></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-3 text-indigo-300">2. 👁️ 由外而內 (Outside-In AI)</h3><p>以「<strong>視覺 AI 代理</strong>」為核心，使用 <span class="keyword"data-keyword="Metropolis">NVIDIA Metropolis</span> 平台。此路徑專注於自動化設施，如：<ul class="list-disc list-inside mt-2 space-y-1 text-slate-400"><li>自動化視覺檢測<li>操作員輔助 (Co-Pilots)<li>廠房營運代理</ul></div></div></section><section class="space-y-6"><h2 class="text-3xl font-bold text-center">📊 <span class="gradient-text bg-gradient-to-r from-teal-300 to-lime-300">Inside-Out vs. Outside-In 比較</span></h2><div class="overflow-x-auto card rounded-lg shadow-lg p-4"><table class="w-full min-w-[600px] text-left"><thead class="bg-gray-700/50"><tr><th class="p-3">特性<th class="p-3">🤖 由內而外 (Inside-Out AI)<th class="p-3">👁️ 由外而內 (Outside-In AI)<tbody><tr class="border-b border-gray-700/50 hover:bg-gray-700/50 transition-colors"><td class="p-3 font-semibold">核心平台<td class="p-3"><span class="keyword"data-keyword="Isaac">NVIDIA Isaac</span><td class="p-3"><span class="keyword"data-keyword="Metropolis">NVIDIA Metropolis</span><tr class="border-b border-gray-700/50 hover:bg-gray-700/50 transition-colors"><td class="p-3 font-semibold">主要媒介<td class="p-3"><span class="keyword"data-keyword="Digital Twins">數位孿生</span> (Digital Twin)<td class="p-3">視覺 AI 代理 (Vision AI Agents)<tr class="border-b border-gray-700/50 hover:bg-gray-700/50 transition-colors"><td class="p-3 font-semibold">應用焦點<td class="p-3">自主機器人 (AMR、機械手臂、人形)<td class="p-3">設施自動化、監控與洞察<tr class="hover:bg-gray-700/50 transition-colors"><td class="p-3 font-semibold">範例<td class="p-3">Isaac Perceptor, Isaac Manipulator<td class="p-3">視覺檢測, 操作員輔助, 廠務代理</table></div></section><section class="space-y-12"><h2 class="text-3xl font-bold text-center">❓ <span class="gradient-text bg-gradient-to-r from-yellow-300 to-orange-300">研討會重點 Q&A</span></h2><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-sky-300">🛰️ NVIDIA Omniverse 3D MODEL</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 給 NVIDIA Omniverse 使用的模型，可以相容什麼建模程式輸出？<div class="text-slate-300 space-y-3"><p>✅ <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 以 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span> (<code>USD</code> / <code>USDC</code> / <code>USDA</code> / <code>USDZ</code>) 當共同格式，所以主流內容工具和工程工具 (像 Maya、Blender、Revit) 都能透過 <span class="keyword"data-keyword="NVIDIA Omniverse Connector">NVIDIA Omniverse Connector</span> 或內建 <code>USD</code> 外掛把場景直接匯出成 <span class="keyword"data-keyword="OpenUSD">USD</span>，送進 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>。<p>換句話說，只要您的軟體能直接輸出 <span class="keyword"data-keyword="OpenUSD">USD</span> (或經由 <span class="keyword"data-keyword="NVIDIA Omniverse Connector">NVIDIA Omniverse Connector</span> / Converter)，基本上都可以進 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 如果想要建立一個場域 (e.g 工廠、辦公室 或室外實際城市 (臺北市)) 怎麼從 NVIDIA Omniverse 建出？<div class="text-slate-300 space-y-3"><p>✅ 承上，建立 <span class="keyword"data-keyword="Digital Twins">Digitial Twin</span> 場景都必須由既有的 3D 模型轉換並組合而成，而建立不同解析度 / 大小的場景則會使用不同工具：<ul class="list-disc list-inside pl-2 space-y-2 text-slate-300"><li><strong>室內 / 工廠 / 辦公室：</strong>把 CAD / BIM (例如 Revit) 直接匯到 <span class="keyword"data-keyword="OpenUSD">USD</span>，載進 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>，就是一個有真實尺寸的場景。<li><strong>戶外 / 整個城市：</strong>可以用 <span class="keyword"data-keyword="Cesium for Omniverse">Cesium for Omniverse</span> 把真實世界的 GIS / 3D Tiles / 航測地形串到 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>，等於一鍵把實際城市 (道路、建物、地形) 放進同一個 <span class="keyword"data-keyword="OpenUSD">USD</span> 場景，拿來做城市級或園區級數位分身。</ul></div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問 NVIDIA Omniverse 的建模，除了 import 即有圖源 (EX:CAD...etc)，有更快速更即時的建模方式嗎？<div class="text-slate-300 space-y-3"><p>✅<ol class="list-decimal list-inside pl-2 space-y-2"><li><strong>實景掃描＋即時重建：</strong><span class="keyword"data-keyword="NVIDIA Isaac ROS nvblox">NVIDIA Isaac ROS nvblox</span> 會用深度相機 / LiDAR 的影像與位姿，邊走邊重建 3D 場景 (mesh) 並產生導航用的 costmap，這些重建出的幾何可以再轉成 <span class="keyword"data-keyword="OpenUSD">USD</span> 拿進 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> / <span class="keyword"data-keyword="Isaac Sim">Isaac Sim</span>。<li><strong>地理資料直接拉進來：</strong><span class="keyword"data-keyword="Cesium for Omniverse">Cesium for Omniverse</span> 可以把整塊真實地理 (城市、工業園區) 直接載成可用場景，而不是手工慢慢建地形建物。<li><strong>AI 生成 3D 物件與場景：</strong><ul class="list-disc list-inside pl-4 mt-2 space-y-1"><li><span class="keyword"data-keyword="Neuralangelo">Neuralangelo</span> (NVIDIA Research, CVPR 2023)：用手機等一般 RGB 影片就能重建高精細 3D 表面，並開源了官方實作。<li><span class="keyword"data-keyword="Instant-NGP / Instant NeRF">Instant-NGP / Instant NeRF</span> (NVIDIA)：用一組相片或影片很快訓練出 NeRF (神經輻射場)，可即時瀏覽和匯出幾何/貼圖。<li><span class="keyword"data-keyword="GET3D">GET3D</span> (NVIDIA Toronto AI Lab)：是一個生成式模型，可以從 2D 影像資料學會整個類別 (車、椅子…)，推論時直接吐出「帶貼圖的 3D mesh」。</ul></ol></div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問 <a href="https://nvlabs.github.io/videomat/"target="_blank"rel="noopener noreferrer"class="qa-link">videomat</a> 這協助快速上材質貼圖的 model 會 release 嗎？<div class="text-slate-300 space-y-3"><p>✅ 目前沒有已經規劃的時程。這部分由 NVIDIA Research 決定，可以持續關注他們的研究進去與 Github。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問如果我們要把 real world 的環境 import 到 NVIDIA Isaac 的話就是可以透過 nvblox 做 3d reconstruction？裏面的機械手臂如果被掃描會不會變成一個無法動的物件？<div class="text-slate-300 space-y-3"><p>✅ 是的。<p>用 <span class="keyword"data-keyword="NVIDIA Isaac ROS nvblox">NVIDIA Isaac ROS nvblox</span> 可以把真實環境用深度+姿態即時重建成 3D mesh / costmap，拿來做導航與避障，之後可轉成 <span class="keyword"data-keyword="OpenUSD">USD</span>，放進 <span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> / <span class="keyword"data-keyword="Omniverse">Omniverse</span> 當「真實工廠/倉庫的底圖」。<p>單純掃描機械手臂只會得到一個靜態 mesh，它在模擬裡只是一塊障礙物並不能動。要讓手臂真的可控，必須要把原廠的 <span class="keyword"data-keyword="URDF / MJCF">URDF / MJCF</span> 匯入 <span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> 的 Importer，把它轉成有關節定義的 <span class="keyword"data-keyword="OpenUSD">USD</span> 機器人，這是我們推薦的流程。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-green-300">⚡ NVIDIA Omniverse PHYSX / RTX</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 可以解釋光線追蹤的基本運作與渲染嗎？<div class="text-slate-300 space-y-3"><p>✅ 光線追蹤的流程是對每個像素從「相機」射出一條 (或多條) 光線，找出它打到的物件、算該點在現有光源下應該多亮、什麼顏色，然後再模擬光線在場景內的反射、折射、間接光等貢獻，最後把多次取樣平均，得到真實的陰影、反射、玻璃、間接光。因為這是在模擬真實光線行為，所以特別適合要高擬真視覺的數位分身和機器人視覺模擬。<p>更多資訊請參考 <a href="https://developer.nvidia.com/rtx/ray-tracing"target="_blank"rel="noopener noreferrer"class="qa-link">https://developer.nvidia.com/rtx/ray-tracing</a>。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 目前資料量不足的問題，如果透過模擬可以處理，那目前模擬的正確率如何呢？我們如何判斷是否正確？那方法是透過將過去物理學家所得知的物理方程，引入進去嗎？<div class="text-slate-300 space-y-3"><p>✅ 可以用高擬真模擬來補足資料。<span class="keyword"data-keyword="Cosmos">NVIDIA COSMOS</span> 和 <span class="keyword"data-keyword="GROOT">GR00T</span> 的大量訓練資料其實就是在 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 的高擬真數位分身環境裡產生的；<span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 本身的光線追蹤與物理引擎都是依照真實物理方程與材質/感測器特性去計算，並非 AI 技術，儘量還原現場行為。實務上，我們會先在模擬裡訓練出策略，再把這套策略丟到實際機器上做小規模驗證，量化誤差 (像是抓取成功率、路徑碰撞率、節拍時間偏差等)。如果這個差距落在業務可以接受的範圍內，就可以視為「準到可以上線」。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問如何整合 Robot 手指上的壓力感測器來跟 Robot 動作一起做模擬？<div class="text-slate-300 space-y-3"><p>✅ 在 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> / <span class="keyword"data-keyword="Isaac Sim">Isaac Sim</span> 的物理引擎 (<span class="keyword"data-keyword="PhysX">PhysX</span>) 裡，每一個接觸點都可以算出力 / 壓力，等同於「虛擬觸覺感測器」。您可以把機械手指的接觸面建成可量力的碰撞幾何，讀出即時的接觸力數值，當成壓力感測器訊號。<p>接著把這些數值透過 Python、OmniGraph 或 ROS2 topic 餵進控制迴路，讓控制器同時看關節角度和「手指現在壓多大力」，去決定要夾更緊還是放鬆，達到閉環觸覺控制。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 想請問 NVIDIA Omniverse 的內部物理模型有機會改嗎？<div class="text-slate-300 space-y-3"><p>✅ 如果您指的是「把底層物理引擎換成自己的 solver」，理論上可以透過客製整合把外部求解器算出的姿態/力回寫到 <span class="keyword"data-keyword="OpenUSD">USD</span> 場景，但那是高難度深度客製。對大部分機器人場景，直接調 <span class="keyword"data-keyword="PhysX">NVIDIA PhysX</span> 參數與擴充邏輯就夠了。<p>一般來說，您可以調整質量、慣量、摩擦係數、關節限制、馬達扭力/速度上限、控制延遲、接觸 solver 精度等，來貼近真實設備或產線。您也可以用自訂 extension/OmniGraph 在模擬步驟中注入額外力場或控制邏輯 (例如吸附力、真空吸嘴、彈性夾爪行為)。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-purple-300">📱 NVIDIA Omniverse APP</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 若透過 LangChain 來調整 LLM 的輸出，這樣的流程可以在 NVIDIA Omniverse 平台上實現嗎？<div class="text-slate-300 space-y-3"><p>✅ <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 是用 NVIDIA Omniverse Kit 架起來的，而 Kit 支援用 Python 寫 extension，去呼叫外部推論服務、REST API 或本地推論流程，然後再把結果即時回寫到場景 (改 <span class="keyword"data-keyword="OpenUSD">USD</span>、更新 UI、驅動機器人等)。NVIDIA 也已經有和 <span class="keyword"data-keyword="LangChain">LangChain</span> 整合的 extension (例如 <code>omni.ai.langchain.*</code>)，用來把 LLM/agent 包成「工具」，讓它可以決定要呼叫哪些 API 並把結果作用在場景裡。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 剛才有注意到 NVIDIA PhysicsNeMo 請問要如何將運用 NVIDIA PhysicsNeMo 上訓練好的模型放上 NVIDIA Omniverse？<div class="text-slate-300 space-y-3"><p>✅ 和上題一樣，NVIDIA Omniverse Kit 本質上是可高度客製化的 Kit extension 平台，所以我們可以寫/掛一個 extension，去載入 <span class="keyword"data-keyword="PhysicsNeMo">NVIDIA PhysicsNeMo</span> 訓練好的物理模型、呼叫推論，然後把結果即時灌回場景 (例如壓力場、流場、熱分布)。<p>NVIDIA 實際是提供了 Kit-CAE， 是一個 NVIDIA Omniverse Kit 的範例 App / extension 組合，用來載入並視覺化工程數值資料：它可以把上述結果轉成 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span> 場景中的資料集，直接在 3D 數位孿生上做流線、體渲染、等值面、點雲標量場等視覺化，並支援互動調參。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問 NVIDIA Omniverse 是否可以用私有的 pre-trained model 來進行 simulate？<div class="text-slate-300 space-y-3"><p>✅ 可以。一般的做法是把您自己的模型 (例如導航策略、抓取策略、瑕疵檢測模型) 以 Python / ROS2 node / extension 的方式接到 <span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> 的模擬迴圈：每個模擬時間步，模擬世界產生感測器資料 (相機影像、深度、觸覺力回饋…)，您的私有模型跑推論，然後把控制指令 (關節角度、速度、力) 寫回模擬中的機器人。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 目前我們部門在使用 NVIDIA Isaac Sim 進行物理 3D 監控，想請問 NVIDIA Isaac Sim 有辦法和 web 串接嗎？上使用者可以透過 web 直接看到 3D 監控？<div class="text-slate-300 space-y-3"><p>✅ 可以。<span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> (本質上也是 NVIDIA Omniverse Kit app) 支援 <span class="keyword"data-keyword="WebRTC">WebRTC</span> 串流模式，您可以在一台有 GPU 的機器上跑 <span class="keyword"data-keyword="Isaac Sim">Isaac Sim</span>，啟動它的 streaming / <span class="keyword"data-keyword="WebRTC">WebRTC</span> client service，然後用瀏覽器端或輕量用戶端直接看到同一個 3D 場景畫面。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 再進一步請教，如果網頁端呈現即時現場 3D 模擬的話，有辦法讓使用者直接點選 3D 物件，反向控制實際現場 3D 環境嗎 (假設現場已有機器人)？<div class="text-slate-300 space-y-3"><p>✅ 可以做到。前端其實就是一個用 NVIDIA Omniverse Kit 做的應用，透過 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 的 streaming 套件把 3D 視角即時串到瀏覽器，不只是單向畫面，而是雙向互動：前端的使用者在瀏覽器裡點場景物件、下指令，這些事件會透過我們自訂的 event 定義回傳到後端的 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>/<span class="keyword"data-keyword="Isaac Sim">Isaac Sim</span> extension，後端可以依照這些互動去更新場景狀態，或直接把動作交給實體機器人執行；同時，場景的狀態更新也可以再同步回前端即時反映。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-yellow-300">🤖 NVIDIA ISAAC SIM TRAINING</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 目前使用 RL 在物理引擎模擬所得到的結果，跟部署到 real world 中間的 gap，己到可以接受的範圍了嗎？之前用六軸手臂模型 train 出來的運動路徑，在實際世界套用時完全不一樣哩<div class="text-slate-300 space-y-3"><p>✅ 現在的 sim-to-real 真的比幾年前成熟很多，特別是接觸型／裝配型任務：NVIDIA 報告在高精度裝配任務中，先在模擬裡大量訓練 (包含視覺或視覺+觸覺訊號) ，再把策略搬到真機，真機成功率已經能做到八成到九成以上，甚至有案例接近九成以上成功率，模擬到實機的落差只剩大約 5%~10%。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問，在有 USD 的情況下想要以 NVIDIA Isaac Lab 在 NVIDIA Isaac Sim 中產生新物件或調整物件的話，有工具能輔助嗎？例如由 Isaac Sim 轉 Isaac Lab 的 code...？<div class="text-slate-300 space-y-3"><p>✅ <span class="keyword"data-keyword="NVIDIA Isaac Lab">NVIDIA Isaac Lab</span> 其實是建在 <span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> 上，兩邊都吃同一個 <span class="keyword"data-keyword="OpenUSD">USD</span> 場景，所以標準流程是先在 <span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> / <span class="keyword"data-keyword="Omniverse">Omniverse</span> 裡把場景或物件 (機械手臂、設備、障礙物…) 改好然後存成 <span class="keyword"data-keyword="OpenUSD">USD</span>，<span class="keyword"data-keyword="NVIDIA Isaac Lab">NVIDIA Isaac Lab</span> 這邊的任務程式會直接載那個 <span class="keyword"data-keyword="OpenUSD">USD</span> 再加上自己的 task/asset 設定 (位置、碰撞、關節限制等) 去跑；基本工作流還是：在 Sim 裡改 → 存 <span class="keyword"data-keyword="OpenUSD">USD</span> → Lab 載同一份 <span class="keyword"data-keyword="OpenUSD">USD</span>。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問是否有模擬無人機飛行場景？<div class="text-slate-300 space-y-3"><p>✅ 有，而且已經有開發者把它做成現成框架。像 <span class="keyword"data-keyword="Pegasus Simulator / OmniDrones">Pegasus Simulator / OmniDrones</span> 這類專案是直接建立在 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> / <span class="keyword"data-keyword="Isaac Sim">Isaac Sim</span> 之上，提供多旋翼 (四軸等) 飛行器的動力學、感測器模擬、以及和 PX4、ArduPilot、ROS 2 的介面，目標就是在寫真環境裡模擬 UAV 的飛控、視覺、避障，再把同一套控制疊到真機。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-lime-300">🦾 NVIDIA ISAAC GR00T</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 除了壓力感測器之外，NVIDIA Isaac GR00T 也會考慮將深度資訊納入訓練資料嗎？<div class="text-slate-300 space-y-3"><p>✅ 利用深度資訊資訊提升操作的穩定性也是可行的方向，但是否會納入 <span class="keyword"data-keyword="GROOT">NVIDIA Isaac GR00T</span> 的模型架構目前無法確認，可能的挑戰在於深度資訊資訊比較難利用 <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span> 模型產生，但這是一個很好的方向。<span class="keyword"data-keyword="GROOT">NVIDIA Isaac GR00T</span> 模型已經開源，若開發者有意願也能可客製化模型架構、並用自家資料微調。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 幻想的動作是如何確保跟實際情況是相同的？<div class="text-slate-300 space-y-3"><p>✅ NVIDIA Isaac GR00T-Dreams 的流程正是用來應對您所提出的問題：<ol class="list-decimal list-inside pl-2 space-y-2"><li>先用少量真實機器人示範 (人類遙操作收的軌跡) 去後訓練 <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span> Predict-2 這個世界模型，讓它學會特定機器人的動作能力與物理限制，接著只要給一張場景影像加一段指令 (例如「把杯子拿起來放進盒子」)，<span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span> Predict-2 就會「作夢」生成機器人執行任務的未來影片。<li>這些夢裡的片段再交給 <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span> Reason (帶推理能力的多模態模型) 篩選，把物理解不通、夾不到、撞到的動作剔除，留下看起來真的完成任務的片段，然後用反向動力學模型（IDM）把這些影片自動轉成可執行的「neural trajectories」。</ol><p>這整個自動資料生產管線讓 NVIDIA 能用 GR00T-Dreams 在約 36 小時內產出足夠的高品質軌跡來訓練 <span class="keyword"data-keyword="GROOT">NVIDIA Isaac GR00T</span> N1.5，而不是花掉原本接近三個月的人力蒐集實機示範。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-red-300">🖥️ NVIDIA AGX</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 真實人類有大小腦，請問後續 NVIDIA 有規劃將軟硬體更細膩區分為大小腦呢？<div class="text-slate-300 space-y-3"><p>✅ NVIDIA 現階段主要專注的是「機器人的大腦」。也就是高層的理解跟決策，例如看環境、懂你要它做什麼、想出下一步要怎麼做，這一層是靠像 <span class="keyword"data-keyword="GROOT">NVIDIA Isaac GR00T</span> 這種大型多模態模型，加上放在機器人身上的 <span class="keyword"data-keyword="Jetson Thor AGX">NVIDIA Jetson / AGX / Thor</span> 這種高算力模組去跑即時推論。<p>而低階、高頻率的控制 (比如馬達伺服、平衡、抖動抑制)，還是由機器人本體自己的控制器 / MCU 去管理。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問邊緣運算最低的系統架構至少要什麼樣的 MCU 或 GPU？<div class="text-slate-300 space-y-3"><p>✅ 要在邊緣端直接跑 AI (影像理解、任務決策、語音/LLM 簡化版)，NVIDIA 把 <span class="keyword"data-keyword="NVIDIA Jetson Orin Nano">NVIDIA Jetson Orin Nano</span> 定義成入門級：它是一塊小型邊緣模組，內建 Arm Cortex-A78AE 多核 CPU 和 Ampere 架構 GPU (512～1024 CUDA core + Tensor Cores)，可提供數十 TOPS 的 AI 推論效能。<p>往上還有 <span class="keyword"data-keyword="Jetson Thor AGX">NVIDIA Jetson AGX Orin / Jetson AGX Thor</span>，這些模組把算力提升到數百 TOPS 甚至數千 TFLOPS FP4 級別，目的是支援高自由度機器人 (像人形機器人或複雜手臂)。實際上要使用甚麼硬體規格仍應該依據任務決定。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-pink-300">🌌 NVIDIA COSMOS</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: NVIDIA Cosmos 材質是貼在 NVIDIA Omniverse 的 3D USD 上面嗎，Cosmos 生成的材質是 random 的嗎，會下 prompt？<div class="text-slate-300 space-y-3"><p>✅ <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span> 是一組 NVIDIA 的「世界基礎模型 (World Foundation Models)」，可以依照文字、單張影像、影片片段或控制訊號 (RGB、深度、語意分割、機器人動作、多視角畫面) 生成或轉換整段場景的影像與影片，包含把模擬畫面轉成更寫實、改變光線/天氣/材質外觀風格，或預測未來多秒、多視角的發生情境；但它本身不是 <span class="keyword"data-keyword="OpenUSD">USD</span>/PBR 材質庫，不會幫您的 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span> 幾何物件自動長出物理材質參數 (粗糙度、金屬度、反射係數等)，這種物理正確的材質與即時/路徑追蹤渲染仍然由 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> (OpenUSD + RTX) 來負責。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: I'm curious about this physical AI model having feedback to make the correction from action tokens.<div class="text-slate-300 space-y-3"><p>✅ <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span>-Predict2.5 有「robot/action-conditioned」版本：您可以把機器人手臂的動作序列 (action tokens) 當成條件，模型就會生成/預測後續影片畫面，等於把「我打算這樣動」→「世界接下來長怎樣」直接學成視覺化世界模型。這類世界模型還能再透過後訓練 (post-training)、LoRA 微調與 <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span>-RL (<span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span> 的強化學習框架) 去加入回饋訊號與獎勵，逐步修正策略，朝「邊看結果邊修動作」的閉環物理 AI。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 如果已經從 NVIDIA Omniverse 建出 (就能丟入 NVIDIA Cosmos 去模擬材質像反射係數等供 Ray Tracing 使用？)<div class="text-slate-300 space-y-3"><p>✅ 工作流程通常是相反方向：先用 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> (以 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span> 場景+RTX 即時/路徑追蹤渲染) 建立高擬真的工廠/城市/機器人環境，並賦予物件真實的 PBR 材質參數 (例如粗糙度、金屬度、反射強度)，這些參數會被 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 的 RTX 渲染器拿來做真實感反射、折射與全域光線傳遞。<p>接著可以把這些模擬結果 (畫面、深度、語意貼標等) 餵進 <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span>-Transfer，讓它把「模擬畫面」變成「照片級真實影片」，或快速產生不同光線/材質觀感/環境條件的版本，以擴增訓練資料。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問在 NVIDIA Cosmos 中，transfer 模組轉換後，會有物理性質嗎？例如：自駕車在轉成出太陽及下雪後，對煞車能力的決策判斷會不同。<div class="text-slate-300 space-y-3"><p>✅ <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span>-Transfer2.5 輸出的是高擬真的影片影格分布 (包含不同天氣、光線等外觀)，它是拿來訓練感知/決策模型用的影像資料增強工具，不是物理模擬器。本質上這些結果已經是「影片」，不再帶有可直接拿來算煞車距離、輪胎摩擦係數之類的物理屬性。那些物理參數還是要在 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> / <span class="keyword"data-keyword="Isaac Sim">Isaac Sim</span> 這種有物理引擎的數位分身裡處理，再把畫面餵給下游模型。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 是否現在有足夠的運動量影片，可以生成和模擬未來的運動模式？<div class="text-slate-300 space-y-3"><p>✅ 如同本次演講提到的，資料蒐集仍然是 <span class="keyword"data-keyword="Physical AI">Physical AI</span> 落地前的最大瓶頸。因此 NVIDIA 提出的世界基礎模型 (world foundation model)，像 <span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span>-Predict2.5，便是利用在 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 中大量蒐集、產生並標註的多視角機器人 / 自駕情境資料進行訓練，能同時理解文字描述、單張影像或一段影片，然後去「預測未來會發生什麼」。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-blue-300">🏙️ NVIDIA METROPOLIS</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 是否有開發多視角融合技術？<div class="text-slate-300 space-y-3"><p>✅ 有的。<span class="keyword"data-keyword="Metropolis">NVIDIA Metropolis</span> 是一個視覺 AI 平台與微服務生態，用來在工廠、零售、物流、智慧城市等空間中部署「多鏡頭感知智慧代理」，它提供多鏡頭/多視角追蹤 (multi-camera tracking) 與多視角 3D 追蹤 (multi-view 3D tracking) 的工作流程與參考程式碼。這些模組會把多台攝影機的偵測、特徵向量與時空資訊融合，給每個人/物件一個全域 ID，讓系統能在不同鏡頭間持續追蹤、穿過遮擋，甚至推估即時 3D 位置。</div></div></div></div><div class="card rounded-lg shadow-lg p-6 md:p-8"><h3 class="text-2xl font-bold mb-6 text-slate-300">📚 MISC (其他資源)</h3><div class="space-y-6"><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 剛剛播放的影片，網路上可以直接找到嗎？<div class="text-slate-300 space-y-3"><p>✅ 本次線上研討會所撥放的影片「Introducing Generative Physical AI」，可以在<a href="https://www.nvidia.com/en-us/events/webinars/generative-physical-ai/"target="_blank"rel="noopener noreferrer"class="qa-link">這裡觀看重播</a>。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問哪裡可以找到或是獲得NVIDIA Omniverse/Cosmos/Isaac...等深入的教育課程？<div class="text-slate-300 space-y-3"><p>✅ NVIDIA 近來推出 <span class="keyword"data-keyword="NVIDIA Learning Path">Learning Path</span> 目標便是將碎片式的資源集結成可循序漸進的學習過程。以下提供三個最相關的 <span class="keyword"data-keyword="NVIDIA Learning Path">Learning Path</span>：<ol class="list-decimal list-inside pl-2 space-y-2"><li><strong><a href="https://developer.nvidia.com/learning-paths/digital-twins-physical-ai/"target="_blank"rel="noopener noreferrer"class="qa-link">Digital Twins / Physical AI Learning Path</a>：</strong>介紹數位分身在實際工廠／資料中心／機器人場景裡怎麼用，帶您從 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span> 基礎開始，一路到 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> Kit 開發、自動合成感測資料、<span class="keyword"data-keyword="Cosmos">NVIDIA Cosmos</span> 世界模型等。<li><strong><a href="https://developer.nvidia.com/learning-paths/openusd/"target="_blank"rel="noopener noreferrer"class="qa-link">Learn OpenUSD Learning Path</a>：</strong>完整教 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span>：Stage / Prim / Layer / Composition / Instancing / Python API，為 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>、工廠數位分身、機器人模擬打底。<li><strong><a href="https://developer.nvidia.com/learning-paths/robotics-isaac/"target="_blank"rel="noopener noreferrer"class="qa-link">Robotics / Isaac Learning Path</a>：</strong>從 <span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> 入門 (機器人模擬、RTX 感測器、合成資料)、到 <span class="keyword"data-keyword="NVIDIA Isaac Lab">NVIDIA Isaac Lab</span> 做強化學習訓練與 sim-to-real、到 <span class="keyword"data-keyword="Isaac">NVIDIA Isaac</span> ROS / Isaac Perceptor (加速 ROS 2 感知、定位、操作)。</ol></div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 今天描述的內容有 NVIDIA 的證照嗎？如果有證照是否有官方教材可以看？<div class="text-slate-300 space-y-3"><p>✅ 有幾種類型可以對應您的需求。首先是 <span class="keyword"data-keyword="NVIDIA DLI">NVIDIA Deep Learning Institute (DLI)</span> 的結業證書：部分 DLI 課程 (像 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 開發與 <span class="keyword"data-keyword="Isaac Sim">NVIDIA Isaac Sim</span> 模擬) 在完成課程與實作後會頒發 NVIDIA DLI Certificate。<p>另外，我們近來也推 NVIDIA 官方認證 (Certification Programs) 例如 <span class="keyword"data-keyword="OpenUSD">OpenUSD</span> 的專業認證，目標是把 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>＋<span class="keyword"data-keyword="OpenUSD">OpenUSD</span> 技能變成企業可驗證的職能。</div></div><div class="bg-gray-800/60 p-5 rounded-lg border border-gray-700"><p class="font-bold text-lg mb-3 text-slate-100">🤔 Q: 請問 NVIDIA Omniverse 可以在商業環境裡面使用嗎？例如：RD 在公司下載後安裝在辦公電腦裡面嘗試開發<div class="text-slate-300 space-y-3"><p>✅ 可以在公司環境裡安裝與做研發，但要注意授權型態。<span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span> 授權條款分成一般使用 (個人/開發測試) 與 <span class="keyword"data-keyword="NVIDIA Omniverse Enterprise">NVIDIA Omniverse Enterprise</span> (正式企業級、多人合作、Nucleus 伺服器長期上線、含企業級支援)。<p>依照目前條款說明：您可以把 <span class="keyword"data-keyword="Omniverse">NVIDIA Omniverse</span>/<span class="keyword"data-keyword="Isaac Sim">Isaac Sim</span> 下載到內部工作站，做內部原型開發、場景編輯或測試，這通常被視為允許的開發/評估用途；但如果要把它當成正式生產環境基礎 (例如整個研發部門長期用 Nucleus 做多人合作、公司流程依賴它)，那就要走 <span class="keyword"data-keyword="NVIDIA Omniverse Enterprise">NVIDIA Omniverse Enterprise</span> 訂閱授權。</div></div></div></div></section><section class="text-center max-w-3xl mx-auto"><h2 class="text-3xl font-bold mb-4">🏁 <span class="gradient-text bg-gradient-to-r from-sky-400 to-purple-400">總結：構築端到端的 AI 工業革命</span></h2><p class="text-lg text-slate-300">NVIDIA 的<span class="keyword"data-keyword="Physical AI">實體 AI</span>策略，從 <span class="keyword"data-keyword="GB200 NVL72">GB200</span> 的強大訓練能力、<span class="keyword"data-keyword="Omniverse">Omniverse</span> 的高保真模擬，到 <span class="keyword"data-keyword="Jetson Thor AGX">Jetson Thor</span> 的高效能部署，正構築一個端到端的閉環生態。這不僅是技術的演進，更是推動全球工業 進入智慧自主新時代的關鍵引擎。</section></main><footer class="fixed bottom-0 left-0 right-0 bg-gray-900/80 backdrop-blur-sm p-4 text-center text-xs text-slate-500 border-t border-gray-700 z-40"><p>Generated by <a href="https://wellstsai.com"target="_blank"rel="noopener noreferrer"class="font-medium text-sky-400 hover:text-sky-300 transition-colors">wellstsai.com</a><p class="mt-1">撰寫日期：2025-10-29</footer><div id="tooltip"role="tooltip"class="card p-3 max-w-[300px]"></div><script>document.addEventListener("DOMContentLoaded",()=>{try{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})}catch(e){console.warn("KaTeX auto-render failed:",e)}const e=document.getElementById("tooltip");if(!e)return;const t={NVIDIA:{title:"NVIDIA (輝達)",desc:"全球頂尖的圖形處理器 (GPU) 與人工智慧 (AI) 運算技術公司。",detail:"此簡報的發表者，正全力推動實體 AI 的發展。"},"Physical AI":{title:"Physical AI (實體 AI)",desc:"一種能理解物理世界、感知環境、並在現實世界中自主行動的 AI。",detail:"被視為繼感知、生成、代理型 AI 後的第四波浪潮。"},Blackwell:{title:"NVIDIA Blackwell 架構",desc:"NVIDIA 最新的 GPU 架構，為 AI 訓練與推論提供前所未有的強大算力。",detail:"簡報中提到的三台電腦均基於此架構。"},"GB200 NVL72":{title:"NVIDIA GB200 NVL72",desc:"基於 Blackwell 架構的 AI 訓練系統，用於大規模基礎模型開發。",detail:"對應實體 AI 架構中的「訓練」(TRAIN) 電腦。"},"DGX/HGX":{title:"NVIDIA DGX / HGX",desc:"NVIDIA 的 AI 伺服器與平台，DGX 為整合系統，HGX 為主機板參考架構。",detail:"早期用於訓練 AI 模型的主力平台。"},"RTX PRO Server":{title:"NVIDIA RTX PRO Server",desc:"用於專業視覺化、渲染與模擬的伺服器，搭載 RTX PRO 專業級 GPU。",detail:"對應實體 AI 架構中的「模擬」(SIMULATE) 電腦。"},"Jetson Thor AGX":{title:"NVIDIA Jetson Thor AGX",desc:"專為機器人、自動駕駛與邊緣 AI 設計的高效能 AI 部署電腦。",detail:"對應實體 AI 架構中的「部署」(DEPLOY) 電腦。"},Omniverse:{title:"NVIDIA Omniverse",desc:"基於 OpenUSD 的 3D 合作與即時模擬平台，用於建立數位孿生。",detail:"在實體 AI 中扮演「模擬」環境的核心角色。"},Cosmos:{title:"NVIDIA Cosmos",desc:"NVIDIA 的世界基礎模型 (World Foundation Model) 平台。",detail:"能理解並生成物理感知的虛擬世界，用於模擬與 AI 推理。"},GROOT:{title:"NVIDIA GROOT 計畫",desc:"NVIDIA 專為人形機器人 (Humanoid Robot) 開發的開放式基礎模型。",detail:"旨在讓機器人能理解自然語言並模仿人類動作。"},Isaac:{title:"NVIDIA Isaac",desc:"NVIDIA 的開放式機器人開發平台，提供模擬、訓練與部署工具。",detail:"包含 Isaac Sim, Isaac Perceptor, Isaac Manipulator 等。"},"Isaac Sim":{title:"NVIDIA Isaac Sim",desc:"NVIDIA 的機器人模擬器，用於產生合成資料與測試 AI 演算法。",detail:"在 Omniverse 平台上運行，是解決資料挑戰的關鍵。"},"Digital Twins":{title:"Digital Twins (數位孿生)",desc:"真實世界物體、流程或系統的虛擬動態副本。",detail:"用於模擬、測試、監控與優化，是工業 4.0 的核心概念。"},OpenUSD:{title:"OpenUSD (Universal Scene Description)",desc:"由 Pixar 開發的開源 3D 場景描述格式，專為大規模合作設計。",detail:"是 NVIDIA Omniverse 平台的核心基礎。"},Metropolis:{title:"NVIDIA Metropolis",desc:"NVIDIA 的 AI 視覺平台，用於智慧城市、零售與工業設施的「由外而內」AI 應用。",detail:"專注於透過攝影機理解與優化現實世界營運。"},"NVIDIA Omniverse Connector":{title:"NVIDIA Omniverse Connector",desc:"一系列外掛程式，用於將主流 3D 內容創作 (DCC) 應用程式（如 Maya, Blender, Revit）連接到 Omniverse 平台。",detail:"允許在不同軟體間即時同步和合作 3D 場景資料。"},"Cesium for Omniverse":{title:"Cesium for Omniverse",desc:"一個將 Cesium 的 3D 地理空間平台（GIS, 3D Tiles）與 NVIDIA Omniverse 整合的擴充套件。",detail:"允許將真實世界的城市模型、地形等大規模地理資料串流到 Omniverse 中，用於城市級數位孿生。"},"NVIDIA Isaac ROS nvblox":{title:"NVIDIA Isaac ROS nvblox",desc:"一個基於 ROS 2 的套件，使用深度相機或 LiDAR 進行即時 3D 重建。",detail:"能夠邊移動邊建立 3D 場景網格 (mesh) 和導航用的 costmap，重建結果可匯出至 Omniverse。"},Neuralangelo:{title:"Neuralangelo (NVIDIA Research)",desc:"NVIDIA 的 AI 研究成果 (CVPR 2023)，可從 2D 影片重建高精細的 3D 表面。",detail:"利用神經網路從一般 RGB 影片（如手機拍攝）生成高細節的 3D 資產。"},"Instant-NGP / Instant NeRF":{title:"Instant-NGP / Instant NeRF (NVIDIA)",desc:"NVIDIA 的快速神經輻射場 (NeRF) 技術，能從少量相片或影片快速訓練出 3D 場景。",detail:"可即時瀏覽並匯出 3D 幾何網格 (mesh)，實現快速實景掃描建模。"},GET3D:{title:"GET3D (NVIDIA Toronto AI Lab)",desc:"NVIDIA 開發的生成式 AI 模型，能從 2D 影像學習並直接生成帶有貼圖的 3D 網格 (mesh)。",detail:"生成的 3D 物件（如車輛、椅子）可立即用於標準 3D 工作流程。"},"URDF / MJCF":{title:"URDF / MJCF",desc:"兩種常見的機器人模型描述格式。URDF (Unified Robot Description Format) 主要用於 ROS；MJCF (MuJoCo XML Format) 用於 MuJoCo 物理引擎。",detail:"這些檔案定義了機器人的關節、連桿、碰撞體等，是將機器人匯入模擬環境的關鍵。"},PhysX:{title:"NVIDIA PhysX",desc:"NVIDIA 的高效能物理模擬引擎，用於遊戲、模擬和數位孿生。",detail:"Omniverse 和 Isaac Sim 皆使用 PhysX 5.x 來模擬剛體動力學、碰撞、流體和軟體等。"},LangChain:{title:"LangChain",desc:"一個用於開發大型語言模型 (LLM) 應用的開源框架。",detail:"它能將 LLM 與外部資料源、API 和其他工具（如 Omniverse）串接，建立複雜的 AI 代理 (Agent)。"},PhysicsNeMo:{title:"NVIDIA PhysicsNeMo",desc:"NVIDIA Modulus（物理機器學習平台）的一部分，用於訓練物理資訊神經網路 (PINN)。",detail:"訓練出的模型能快速預測複雜的物理現象（如流體、熱傳），可整合至 Omniverse 進行視覺化。"},WebRTC:{title:"WebRTC (Web Real-Time Communication)",desc:"一項開源技術，允許網頁瀏覽器之間進行即時的音訊、視訊和資料傳輸。",detail:"Omniverse/Isaac Sim 利用 WebRTC 將 3D 模擬畫面即時串流到瀏覽器端，實現遠端監控與互動。"},RL:{title:"RL (Reinforcement Learning)",desc:"強化學習，一種機器學習方法，AI 代理 (Agent) 透過與環境互動並獲得獎勵或懲罰來學習最佳策略。",detail:"常用於在 Isaac Sim 中訓練機器人執行複雜任務（如抓取、行走）。"},"NVIDIA Isaac Lab":{title:"NVIDIA Isaac Lab",desc:"基於 Isaac Sim 的機器人強化學習 (RL) 應用程式。",detail:"提供高效能、物理精確的 RL 訓練環境，專為機器人基礎模型（如 GROOT）設計。"},"Pegasus Simulator / OmniDrones":{title:"Pegasus Simulator / OmniDrones",desc:"建立在 NVIDIA Isaac Sim 上的開源專案，專門用於無人機 (UAV) 模擬。",detail:"提供多旋翼飛行器的動力學模擬，並支援與 PX4、ROS 2 等飛控軟體整合。"},"NVIDIA Jetson Orin Nano":{title:"NVIDIA Jetson Orin Nano",desc:"NVIDIA Jetson 系列的入門級邊緣 AI 運算模組。",detail:"提供數十 TOPS 的 AI 推論效能，適用於需要 AI 影像理解的入門級機器人或邊緣設備。"},"NVIDIA Learning Path":{title:"NVIDIA Learning Path",desc:"NVIDIA 官方提供的結構化學習資源，將碎片化的教學影片、文件和實作課程整合成循序漸進的學習路徑。",detail:"涵蓋 OpenUSD、Digital Twins、Robotics/Isaac 等主題。"},"NVIDIA DLI":{title:"NVIDIA DLI (Deep Learning Institute)",desc:"NVIDIA 深度學習學院，提供 AI、資料科學和高效能運算 (HPC) 的實作培訓。",detail:"完成課程後會頒發結業證書 (DLI Certificate)。"},"NVIDIA Omniverse Enterprise":{title:"NVIDIA Omniverse Enterprise",desc:"NVIDIA Omniverse 的企業級授權版本。",detail:"提供正式的商業部署授權、多人合作伺服器 (Nucleus) 以及企業級技術支援服務。"}};function i(t){const i=window.innerWidth,s=window.innerHeight,a=.9*i;e.style.maxWidth=`${Math.min(a,300)}px`;const I=e.offsetWidth,l=e.offsetHeight;let n=t.pageX+15,d=t.pageY+15;n+I>i-15&&(n=t.pageX-I-15),t.clientY+l>s-15&&(d=t.pageY-l-15),d<window.scrollY+15&&(d=t.pageY+15),n<15&&(n=t.pageX+15),e.style.left=`${n}px`,e.style.top=`${d}px`}document.querySelectorAll(".keyword").forEach(s=>{const a=s.getAttribute("data-keyword");t[a]?(s.addEventListener("mouseenter",s=>{!function(i){const s=t[i];if(!s)return void(e.innerHTML="<p>找不到定義</p>");let a=`<h3 class="font-bold text-base mb-1 gradient-text bg-gradient-to-r from-sky-300 to-purple-300">${s.title}</h3>`;a+=`<p class="text-sm text-slate-200 mb-2">${s.desc}</p>`,s.detail&&(a+=`<p class="text-xs text-slate-400 border-t border-gray-600 pt-2">${s.detail}</p>`),e.innerHTML=a;try{renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}catch(e){console.warn("KaTeX render in tooltip failed:",e)}}(a),e.style.opacity="1",e.style.visibility="visible",i(s)}),s.addEventListener("mousemove",i),s.addEventListener("mouseleave",()=>{e.style.opacity="0",e.style.visibility="hidden"})):s.classList.remove("keyword")})})</script>